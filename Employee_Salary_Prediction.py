# -*- coding: utf-8 -*-
"""EmployeeSalaryPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GYnPr2QwSMmX76UgTbOvd2jFk4josyJT

#💼Employee Salary Prediction using adult csv

**Data Loading**
"""

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#Load the dataset
data=pd.read_csv("/content/adult.csv")

"""**Data Inspection**"""

data

data.shape

data.head()

data.tail()

data.info()

import seaborn as sns

plt.figure(figsize = (10, 5))
sns.countplot(x = data['income'])
plt.show()

# Display basic descriptive statistics for numerical features
print(data.describe())

data.describe(include='all')

data.isna().sum()

#finding null values
for column in data.columns:
    if (data[column] == '?').any():
        print(f"Column '{column}' has {(data[column] == '?').sum()} missing values ('?').")

data.nunique()

print(data.gender.value_counts())

print(data['education'].value_counts())

print(data['workclass'].value_counts())

print(data['marital-status'].value_counts())

"""#Handling Missing Values and Redundant Categories"""

data['occupation'] = data['occupation'].replace({'?':'others'})

print(data.occupation.value_counts())

data['native-country'] = data['native-country'].replace({'?':'others'})

data

print(data.occupation.value_counts())

data['workclass'] = data['workclass'].replace({'?':'others'})

print(data['workclass'].value_counts())

"""#Data Selection and Filtering"""

# Removing Redundant Workclass Categories
data=data[data['workclass'] != 'Without-pay' ]
data=data[data['workclass'] != 'Never-worked' ]

print(data['workclass'].value_counts())

data.shape

# Removing Redundant Education Categories
data=data[data['education'] != '5th-6th' ]
data=data[data['education'] != '1st-4th' ]
data=data[data['education'] != 'Preschool' ]

print(data['education'].value_counts())

data.shape

data.iloc[4:20, 1:5]

education_mapping = data.groupby('education')['educational-num'].unique()
print(education_mapping)

#redundancy
data.drop(columns=['education'],inplace=True)

"""#Data Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Numerical Features: Histogram
num_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']

for col in num_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(data[col], bins=30, kde=True)
    plt.title(f'{col} Distribution')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.show()

# Categorical Features: Countplot
cat_cols = ['workclass', 'marital-status', 'occupation',
            'relationship', 'race', 'gender', 'native-country', 'income']

for col in cat_cols:
    plt.figure(figsize=(12, 5))
    sns.countplot(y=col, data=data, order=data[col].value_counts().index)
    plt.title(f'{col} Distribution')
    plt.xlabel('Count')
    plt.ylabel(col)
    plt.grid(True)
    plt.show()

"""#Outlier Handling

"""

#outlier
import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()

# Removing Outliers from 'age' Column
data= data[(data['age']<=75)& (data['age']>=17)]
plt.boxplot(data['age'])
plt.show()

plt.boxplot(data['capital-gain'])
plt.show()

plt.boxplot(data['educational-num'])
plt.show()

#Removing Outliers from 'educational-num' Column
data=data[(data['educational-num']<=16)&(data['educational-num']>=5)]
plt.boxplot(data['educational-num'])
plt.show()

plt.boxplot(data['hours-per-week'])
plt.show()

data.shape

"""#Data Preprocessing for Modeling"""

data

"""**Categorical Data Encoding -> sklearn label encoder**"""

#Label Encoding
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# List of categorical columns to encode
categorical_cols = ['workclass', 'marital-status', 'occupation',
                    'relationship', 'race', 'gender', 'native-country']

# Dictionary to store encoders for later use
encoders = {}

# Apply Label Encoding and print mappings
for col in categorical_cols:
    enc = LabelEncoder()
    data[col] = enc.fit_transform(data[col])
    encoders[col] = enc  # Save encoder for possible reverse transform

    # Print mapping
    print(f"🔠 {col} mapping:")
    for original_value, encoded_value in zip(enc.classes_, enc.transform(enc.classes_)):
        print(f"  {original_value} → {encoded_value}")
    print("\n" + "-"*40 + "\n")

data.dtypes

"""**Normalization / Standardization**"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Select only numerical columns for scaling
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns

# Fit and transform the numerical data
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

# Display the transformed data
display(data.head())

x=data.drop(columns=['income']) #input
y=data['income']

#y = f(x) - independent variable(features)
#y - dependent variable (target variable)

x

y

"""#Data Splitting and Balancing

**Split the Dataset into Training and Test Sets**
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=23, stratify=y)

x_train.shape

x_train.head()

x_test.shape

"""**Balance the dataset**"""

import seaborn as sns
plt.figure(figsize = (10, 5))
sns.countplot(x = y_train)
plt.show()

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
x_balanced, y_balanced = smote.fit_resample(x_train, y_train)

# Display the new class distribution
plt.figure(figsize = (10, 5))
sns.countplot(x = y_balanced)
plt.title('Class Distribution After SMOTE')
plt.show()

print("Original dataset shape:", x_train.shape, y_train.shape)
print("Resampled dataset shape:", x_balanced.shape, y_balanced.shape)

"""#Model Training and Evaluation

**Import Models**
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

"""**KNeighborsClassifier**"""

#Machine Learning Algorithm
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(x_train,y_train) #input and output training data
predict=knn.predict(x_test)
print("KNNn Accuracy:", accuracy_score(y_test, predict))
print("KNN Classification Report:\n", classification_report(y_test, predict))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm_knn = confusion_matrix(y_test, predict)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_knn)
disp.plot()
plt.title('KNN Confusion Matrix')
plt.show()

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(x_train,y_train)
predict1=lr.predict(x_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, predict1))
print("Logistic Regression Classification Report:\n", classification_report(y_test, predict1))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm_lr = confusion_matrix(y_test, predict1)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr)
disp.plot()
plt.title('Logistic Regression Confusion Matrix')
plt.show()

"""**MLP Classifier**"""

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), random_state=2, max_iter=2000)
clf.fit(x_train,y_train)
predict2=clf.predict(x_test)
print("MLP Classifier Accuracy:", accuracy_score(y_test, predict2))
print("MLP Classifier Classification Report:\n", classification_report(y_test, predict2))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm_mpl = confusion_matrix(y_test, predict2)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_mpl)
disp.plot()
plt.title('MPL Classifier Confusion Matrix')
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {
           "LogisticRegression": LogisticRegression(),
           "RandomForest": RandomForestClassifier(),
           "KNN": KNeighborsClassifier(),
           "SVM": SVC(),
           "GradientBoosting": GradientBoostingClassifier()
}

results = {}

for name, model in models.items():
  pipe = Pipeline([
       ('scaler', StandardScaler()),
        ('model', model)
        ])
  pipe.fit(X_train, y_train)
  y_pred = pipe.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  results[name] = acc
  print(f"{name} Accuracy: {acc:.4f}")
  print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt

plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title("Model Comparison")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""#Model Saving and Prediction with New Data"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"{name}: {acc:.4f}")

# Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\n✅ Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model, "best_model.pkl")
print("✅ Saved best model as best_model.pkl")

import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

# Load the saved model
best_model = joblib.load("best_model.pkl")

# Load the original data to fit scalers and encoders
# This is important to ensure consistent preprocessing
original_data = pd.read_csv("/content/adult.csv")

# Reapply the data cleaning and preprocessing steps to a temporary copy of the original data
temp_data = original_data.copy()

# Handle missing values
temp_data['occupation'] = temp_data['occupation'].replace({'?':'others'})
temp_data['native-country'] = temp_data['native-country'].replace({'?':'others'})
temp_data['workclass'] = temp_data['workclass'].replace({'?':'others'})

# Removing redundant categories
temp_data = temp_data[temp_data['workclass'] != 'Without-pay' ]
temp_data = temp_data[temp_data['workclass'] != 'Never-worked' ]
temp_data = temp_data[temp_data['education'] != '5th-6th' ]
temp_data = temp_data[temp_data['education'] != '1st-4th' ]
temp_data = temp_data[temp_data['education'] != 'Preschool' ]

# Drop redundant education column
temp_data.drop(columns=['education'], inplace=True)

# Apply Label Encoding and fit encoders on the temporary data
categorical_cols = ['workclass', 'marital-status', 'occupation',
                    'relationship', 'race', 'gender', 'native-country']
encoders = {}
for col in categorical_cols:
    enc = LabelEncoder()
    temp_data[col] = enc.fit_transform(temp_data[col])
    encoders[col] = enc

# Fit the MinMaxScaler on the temporary data
scaler = MinMaxScaler()
numerical_cols = temp_data.select_dtypes(include=['int64', 'float64']).columns
scaler.fit(temp_data[numerical_cols]) # Fit the scaler

def preprocess_single_input(input_data, encoders, scaler, numerical_cols):
    """
    Applies the same preprocessing steps to a single input data point.

    Args:
        input_data (dict): A dictionary containing the input features.
        encoders (dict): A dictionary of fitted LabelEncoders.
        scaler (MinMaxScaler): A fitted MinMaxScaler.
        numerical_cols (list): List of numerical column names.

    Returns:
        pd.DataFrame: The preprocessed input data as a DataFrame.
    """
    # Create a DataFrame from the input dictionary
    input_df = pd.DataFrame([input_data])

    # Apply Label Encoding to categorical columns
    for col, encoder in encoders.items():
        if col in input_df.columns:
             # Handle potential unseen labels by transforming only if the label is in known classes
            input_df[col] = input_df[col].apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1) # Or some other value indicating unseen

    # Apply Min-Max Scaling to numerical columns
    input_df[numerical_cols] = scaler.transform(input_df[numerical_cols])

    return input_df

# Example single input (replace with your desired values)
single_input = {
    'age': 35,
    'workclass': 'Private',
    'fnlwgt': 250000,
    'educational-num': 13, # Corresponds to Bachelors
    'marital-status': 'Married-civ-spouse',
    'occupation': 'Exec-managerial',
    'relationship': 'Husband',
    'race': 'White',
    'gender': 'Male',
    'capital-gain': 2000,
    'capital-loss': 0,
    'hours-per-week': 40,
    'native-country': 'United-States'
}

# Preprocess the single input
preprocessed_input = preprocess_single_input(single_input, encoders, scaler, numerical_cols)

# Make a prediction using the best model
prediction = best_model.predict(preprocessed_input)

print(f"\nPrediction for the single input: {prediction[0]}")

"""#Streamlit Application Deployment"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# from sklearn.preprocessing import MinMaxScaler, LabelEncoder
# import numpy as np # Import numpy for handling potential unseen values
# 
# # Load the saved model
# model = joblib.load("best_model.pkl")
# 
# # Load the original data to fit scalers and encoders
# # This is crucial for consistent preprocessing
# try:
#     original_data = pd.read_csv("/content/adult.csv")
# except FileNotFoundError:
#     st.error("Error: adult.csv not found. Please upload the dataset to /content/adult.csv")
#     st.stop()
# 
# 
# # --- Preprocessing Setup (Fit scalers and encoders on the original data) ---
# temp_data_for_fitting = original_data.copy()
# 
# # Handle missing values for fitting
# temp_data_for_fitting['occupation'] = temp_data_for_fitting['occupation'].replace({'?':'others'})
# temp_data_for_fitting['native-country'] = temp_data_for_fitting['native-country'].replace({'?':'others'})
# temp_data_for_fitting['workclass'] = temp_data_for_fitting['workclass'].replace({'?':'others'})
# 
# # Removing redundant categories for fitting
# temp_data_for_fitting = temp_data_for_fitting[temp_data_for_fitting['workclass'] != 'Without-pay' ]
# temp_data_for_fitting = temp_data_for_fitting[temp_data_for_fitting['workclass'] != 'Never-worked' ]
# temp_data_for_fitting = temp_data_for_fitting[temp_data_for_fitting['education'] != '5th-6th' ]
# temp_data_for_fitting = temp_data_for_fitting[temp_data_for_fitting['education'] != '1st-4th' ]
# temp_data_for_fitting = temp_data_for_fitting[temp_data_for_fitting['education'] != 'Preschool' ]
# 
# # Drop redundant education column for fitting
# temp_data_for_fitting.drop(columns=['education'], inplace=True)
# 
# 
# # Fit Label Encoders
# categorical_cols_to_encode = ['workclass', 'marital-status', 'occupation',
#                               'relationship', 'race', 'gender', 'native-country']
# encoders = {}
# for col in categorical_cols_to_encode:
#     enc = LabelEncoder()
#     # Fit on combined data to handle all possible categories
#     all_categories = original_data[col].unique().tolist()
#     if '?' in all_categories:
#       all_categories.remove('?')
#       all_categories.append('others')
#     # Ensure consistent order of classes
#     all_categories.sort()
#     enc.fit(all_categories)
#     encoders[col] = enc
# 
# # Fit MinMaxScaler
# numerical_cols_for_scaling = temp_data_for_fitting.select_dtypes(include=['int64', 'float64']).columns
# scaler = MinMaxScaler()
# scaler.fit(temp_data_for_fitting[numerical_cols_for_scaling])
# 
# 
# # --- Preprocessing Function ---
# def preprocess_input(input_df, encoders, scaler, numerical_cols):
#     """
#     Applies the same preprocessing steps to the input DataFrame from Streamlit.
# 
#     Args:
#         input_df (pd.DataFrame): DataFrame containing the input features.
#         encoders (dict): A dictionary of fitted LabelEncoders.
#         scaler (MinMaxScaler): A fitted MinMaxScaler.
#         numerical_cols (list): List of numerical column names used for scaling.
# 
#     Returns:
#         pd.DataFrame: The preprocessed input data as a DataFrame.
#     """
#     processed_df = input_df.copy()
# 
#     # Handle missing values (assuming '?' is replaced by 'others' in the form or handled here)
#     # For simplicity, we'll assume the form handles basic replacement or we handle it here
#     for col in ['workclass', 'occupation', 'native-country']:
#         if col in processed_df.columns:
#             processed_df[col] = processed_df[col].replace({np.nan: 'others', '?': 'others'}) # Handle potential NaN from form as well
# 
#     # Apply Label Encoding
#     for col, encoder in encoders.items():
#         if col in processed_df.columns:
#             # Ensure consistent order and handle potential unseen labels
#             processed_df[col] = processed_df[col].apply(lambda x: encoder.transform([x])[0] if x in encoder.classes_ else -1) # Handle unseen with -1 or another strategy
# 
#     # Ensure numerical columns are present before scaling
#     for col in numerical_cols:
#         if col not in processed_df.columns:
#              processed_df[col] = 0 # Add missing numerical columns with a default value if necessary
# 
#     # Apply Min-Max Scaling
#     processed_df[numerical_cols] = scaler.transform(processed_df[numerical_cols])
# 
# 
#     # Ensure all required columns are present in the processed_df in the correct order
#     # This is important if the model expects features in a specific order
#     # Get the list of columns the model was trained on (excluding the target)
#     model_trained_cols = temp_data_for_fitting.drop(columns=['income']).columns.tolist()
# 
#     # Add any missing columns to the processed_df with a default value (e.g., 0)
#     for col in model_trained_cols:
#         if col not in processed_df.columns:
#             processed_df[col] = 0
# 
#     # Reorder columns to match the training data
#     processed_df = processed_df[model_trained_cols]
# 
# 
#     return processed_df
# 
# 
# # 🔹 Set page config
# st.set_page_config(page_title="Employee Salary Classification", page_icon="💼", layout="centered")
# 
# # 🔹 Custom CSS for styling
# st.markdown("""
#     <style>
#     /* Gradient background */
#     body {
#         background: linear-gradient(to right, #e0c3fc, #8ec5fc);
#     }
# 
#     /* Main content area styling */
#     .main {
#         background-color: rgba(255, 255, 255, 0.85);
#         padding: 2rem;
#         border-radius: 15px;
#         box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
#     }
# 
#     /* Title style */
#     h1 {
#         color: #4B0082;
#         font-weight: 700;
#         text-shadow: 1px 1px 2px #fff;
#     }
# 
#     /* Button style */
#     .stButton > button {
#         background: linear-gradient(to right, #FF512F, #DD2476);
#         color: white;
#         border: none;
#         border-radius: 12px;
#         padding: 0.75rem 2rem;
#         font-weight: bold;
#         font-size: 16px;
#     }
# 
#     .stButton > button:hover {
#         background: linear-gradient(to right, #DD2476, #FF512F);
#         color: white;
#     }
# 
#     /* Sidebar header */
#     .sidebar .sidebar-content {
#         background: linear-gradient(to bottom, #fceabb, #f8b500);
#     }
# 
#     /* Smaller tweaks */
#     .stMarkdown h3 {
#         color: #2e2e2e;
#         margin-top: 1rem;
#     }
#     </style>
# """, unsafe_allow_html=True)
# 
# 
# # App Title
# st.title("💼 Employee Salary Classification App")
# st.markdown("##### Predict whether an employee earns >50K or ≤50K based on input features.")
# 
# # Sidebar Inputs (Update these to match the features used by your model)
# # Refer to the columns in your 'data' DataFrame after preprocessing in the notebook
# st.sidebar.header("📋 Input Employee Details")
# 
# # Get unique values for dropdowns from the original data (after replacing '?')
# workclass_options = sorted(original_data['workclass'].replace({'?':'others'}).unique().tolist())
# marital_status_options = sorted(original_data['marital-status'].unique().tolist())
# occupation_options = sorted(original_data['occupation'].replace({'?':'others'}).unique().tolist())
# relationship_options = sorted(original_data['relationship'].unique().tolist())
# race_options = sorted(original_data['race'].unique().tolist())
# gender_options = sorted(original_data['gender'].unique().tolist())
# native_country_options = sorted(original_data['native-country'].replace({'?':'others'}).unique().tolist())
# education_num_min = int(original_data['educational-num'].min())
# education_num_max = int(original_data['educational-num'].max())
# 
# 
# age = st.sidebar.slider("Age", int(original_data['age'].min()), int(original_data['age'].max()), 30) # Use original data min/max
# workclass = st.sidebar.selectbox("Workclass", workclass_options)
# fnlwgt = st.sidebar.number_input("Fnlwgt", int(original_data['fnlwgt'].min()), int(original_data['fnlwgt'].max()), 189664) # Use original data min/max, mean as default
# educational_num = st.sidebar.slider("Educational Number", education_num_min, education_num_max, 10)
# marital_status = st.sidebar.selectbox("Marital Status", marital_status_options)
# occupation = st.sidebar.selectbox("Occupation", occupation_options)
# relationship = st.sidebar.selectbox("Relationship", relationship_options)
# race = st.sidebar.selectbox("Race", race_options)
# gender = st.sidebar.selectbox("Gender", gender_options)
# capital_gain = st.sidebar.number_input("Capital Gain", int(original_data['capital-gain'].min()), int(original_data['capital-gain'].max()), 0)
# capital_loss = st.sidebar.number_input("Capital Loss", int(original_data['capital-loss'].min()), int(original_data['capital-loss'].max()), 0)
# hours_per_week = st.sidebar.slider("Hours per week", int(original_data['hours-per-week'].min()), int(original_data['hours-per-week'].max()), 40)
# native_country = st.sidebar.selectbox("Native Country", native_country_options)
# 
# 
# # Build input DataFrame
# input_data = {
#     'age': age,
#     'workclass': workclass,
#     'fnlwgt': fnlwgt,
#     'educational-num': educational_num,
#     'marital-status': marital_status,
#     'occupation': occupation,
#     'relationship': relationship,
#     'race': race,
#     'gender': gender,
#     'capital-gain': capital_gain,
#     'capital-loss': capital_loss,
#     'hours-per-week': hours_per_week,
#     'native-country': native_country
# }
# input_df = pd.DataFrame([input_data])
# 
# 
# # Show Input Data
# st.write("### 🔎 Input Data")
# st.dataframe(input_df)
# 
# # Predict Button
# if st.button("Predict Salary Class"):
#     # Preprocess the input data
#     preprocessed_input_df = preprocess_input(input_df.copy(), encoders, scaler, numerical_cols_for_scaling)
# 
#     # Make a prediction using the best model
#     prediction = model.predict(preprocessed_input_df)
# 
#     st.success(f"✅ Prediction: **{prediction[0]}**")
# 
# # Divider
# st.markdown("---")
# 
# # Batch Prediction Section (Update this to also use the preprocessing function)
# st.markdown("### 📂 Batch Prediction")
# uploaded_file = st.file_uploader("Upload a CSV file for batch prediction", type="csv")
# 
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("Uploaded Data Preview:", batch_data.head())
# 
#     # Preprocess the batch data
#     preprocessed_batch_data = preprocess_input(batch_data.copy(), encoders, scaler, numerical_cols_for_scaling)
# 
#     # Make batch predictions
#     batch_preds = model.predict(preprocessed_batch_data)
#     batch_data['PredictedClass'] = batch_preds
#     st.write("✅ Predictions:")
#     st.dataframe(batch_data.head())
# 
#     # Download CSV
#     csv = batch_data.to_csv(index=False).encode('utf-8')
#     st.download_button("⬇️ Download Predictions CSV", csv, file_name='predicted_classes.csv', mime='text/csv')

!pip install streamlit

!wget -q -O - ipv4.icanhazip.com

!streamlit run app.py & npx localtunnel --port 8501